{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qkeras as q\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from qkeras import utils as u\n",
    "import numpy as np\n",
    "import larq as lq\n",
    "from tqdm import tqdm\n",
    "import cv2 \n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_quicknet = \"../../quicknet_weights.h5\"\n",
    "path_quicknet_small = \"../../quicknet_small_weights.h5\"\n",
    "path_quicknet_large = \"../../quicknet_large_weights.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create random dataset\n",
    "test_data = np.random.randint(low=0, high=254, size=(128, 1, 224, 224, 3))\n",
    "\n",
    "# load data from ImageNET dataset\n",
    "def loadImageNetData(path, image_num):\n",
    "    file_list = os.listdir(path)\n",
    "    file_list.sort()\n",
    "    print(len(file_list))\n",
    "    file_list = file_list[0:image_num]\n",
    "    im = []\n",
    "    for i in tqdm(file_list):\n",
    "        img = cv2.imread(os.path.join(path, i))\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        im.append(img)\n",
    "    im = np.asarray(im)\n",
    "    im = np.expand_dims(im, axis=1)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_qkeras_residual(model, filter_index):\n",
    "    model.add(q.QActivation(\"binary\"))\n",
    "    model.add(q.QConv2D(filters[filter_index], (3, 3), activation=\"relu\", kernel_quantizer=\"binary\",\n",
    "                kernel_initializer=\"glorot_normal\", padding=\"same\", use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5))\n",
    "    return model\n",
    "\n",
    "def add_qkeras_transistion(model, strides, filter_index):\n",
    "    model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=strides, strides=1))\n",
    "    model.add(tf.keras.layers.DepthwiseConv2D((3, 3), padding=\"same\",\n",
    "                strides=strides, trainable=False, use_bias=False))\n",
    "    model.add(q.QConv2D(filters[filter_index], (1, 1), kernel_initializer=\"glorot_normal\", use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def larq_add_qkeras_residual(model, filter_index):\n",
    "    model.add(q.QActivation(\"binary\"))\n",
    "    model.add(lq.layers.QuantConv2D(filters[filter_index], (3, 3), activation=\"relu\", kernel_quantizer=lq.quantizers.SteSign(clip_value=1.25),\n",
    "                kernel_constraint=lq.constraints.WeightClip(clip_value=1.25), kernel_initializer=\"glorot_normal\", padding=\"same\", use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5))\n",
    "    return model\n",
    "\n",
    "def larq_add_qkeras_transistion(model, strides, filter_index):\n",
    "    model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=strides, strides=1))\n",
    "    model.add(tf.keras.layers.DepthwiseConv2D((3, 3), padding=\"same\",\n",
    "                strides=strides, trainable=False, use_bias=False))\n",
    "    model.add(lq.layers.QuantConv2D(filters[filter_index], (1, 1), kernel_initializer=\"glorot_normal\", use_bias=False))\n",
    "    model.add(tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = ((64, 128, 256, 512))\n",
    "qkeras_quickNet = tf.keras.models.Sequential() \n",
    "\n",
    "# INPUT\n",
    "qkeras_quickNet.add(tf.keras.layers.InputLayer(input_shape=(224, 224, 3)))\n",
    "\n",
    "# STEM MODULE\n",
    "qkeras_quickNet.add(q.QConv2D(filters[0] // 4, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\", strides=2, use_bias=False))\n",
    "qkeras_quickNet.add(tf.keras.layers.BatchNormalization())\n",
    "qkeras_quickNet.add(tf.keras.layers.Activation(\"relu\"))\n",
    "qkeras_quickNet.add(q.QDepthwiseConv2D((3, 3),padding=\"same\",strides=2, use_bias=False))\n",
    "qkeras_quickNet.add(tf.keras.layers.BatchNormalization(scale=False, center=False))\n",
    "qkeras_quickNet.add(q.QConv2D(filters[0], 1, kernel_initializer=\"he_normal\", use_bias=False))\n",
    "qkeras_quickNet.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "for i in range(0,3):\n",
    "    # RESIDUAL MODULE\n",
    "    for j in range(0,4):\n",
    "        add_qkeras_residual(qkeras_quickNet, filter_index=i)\n",
    "\n",
    "    # TRANSITION MODULE\n",
    "    add_qkeras_transistion(qkeras_quickNet, strides=2, filter_index=i+1)\n",
    "    \n",
    "for i in range(0,4):\n",
    "        add_qkeras_residual(qkeras_quickNet, filter_index=3)\n",
    "        \n",
    "# FINAL\n",
    "qkeras_quickNet.add(tf.keras.layers.Activation(\"relu\"))\n",
    "qkeras_quickNet.add(tf.keras.layers.AveragePooling2D(pool_size=(7,7)))\n",
    "qkeras_quickNet.add(tf.keras.layers.Flatten())\n",
    "qkeras_quickNet.add(q.QDense(1000, kernel_initializer=\"glorot_normal\"))\n",
    "qkeras_quickNet.add(tf.keras.layers.Activation(\"softmax\", dtype=\"float32\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = ((64, 128, 256, 512))\n",
    "larq_quickNet = tf.keras.models.Sequential() \n",
    "\n",
    "# INPUT\n",
    "larq_quickNet.add(tf.keras.layers.InputLayer(input_shape=(224, 224, 3)))\n",
    "\n",
    "# STEM MODULE\n",
    "larq_quickNet.add(lq.layers.QuantConv2D(filters[0] // 4, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\", strides=2, use_bias=False))\n",
    "larq_quickNet.add(tf.keras.layers.BatchNormalization())\n",
    "larq_quickNet.add(tf.keras.layers.Activation(\"relu\"))\n",
    "larq_quickNet.add(lq.layers.QuantDepthwiseConv2D((3, 3),padding=\"same\",strides=2, use_bias=False))\n",
    "larq_quickNet.add(tf.keras.layers.BatchNormalization(scale=False, center=False))\n",
    "larq_quickNet.add(lq.layers.QuantConv2D(filters[0], 1, kernel_initializer=\"he_normal\", use_bias=False))\n",
    "larq_quickNet.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "for i in range(0,3):\n",
    "    # RESIDUAL MODULE\n",
    "    for j in range(0,4):\n",
    "        larq_add_qkeras_residual(larq_quickNet, filter_index=i)\n",
    "\n",
    "    # TRANSITION MODULE\n",
    "    larq_add_qkeras_transistion(larq_quickNet, strides=2, filter_index=i+1)\n",
    "    \n",
    "for i in range(0,4):\n",
    "        larq_add_qkeras_residual(larq_quickNet, filter_index=3)\n",
    "        \n",
    "# FINAL\n",
    "larq_quickNet.add(tf.keras.layers.Activation(\"relu\"))\n",
    "larq_quickNet.add(tf.keras.layers.AveragePooling2D(pool_size=(7,7)))\n",
    "larq_quickNet.add(tf.keras.layers.Flatten())\n",
    "larq_quickNet.add(lq.layers.QuantDense(1000, kernel_initializer=\"glorot_normal\"))\n",
    "larq_quickNet.add(tf.keras.layers.Activation(\"softmax\", dtype=\"float32\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = ((32, 64, 256, 512))\n",
    "qkeras_quickNet_small = tf.keras.models.Sequential() \n",
    "\n",
    "# INPUT\n",
    "qkeras_quickNet_small.add(tf.keras.layers.InputLayer(input_shape=(224, 224, 3)))\n",
    "\n",
    "# STEM MODULE\n",
    "qkeras_quickNet_small.add(q.QConv2D(filters[0] // 4, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\", strides=2, use_bias=False))\n",
    "qkeras_quickNet_small.add(tf.keras.layers.BatchNormalization())\n",
    "qkeras_quickNet_small.add(tf.keras.layers.Activation(\"relu\"))\n",
    "qkeras_quickNet_small.add(q.QDepthwiseConv2D((3, 3),padding=\"same\",strides=2, use_bias=False))\n",
    "qkeras_quickNet_small.add(tf.keras.layers.BatchNormalization(scale=False, center=False))\n",
    "qkeras_quickNet_small.add(q.QConv2D(filters[0], 1, kernel_initializer=\"he_normal\", use_bias=False))\n",
    "qkeras_quickNet_small.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "for i in range(0,3):\n",
    "    # RESIDUAL MODULE\n",
    "    for j in range(0,4):\n",
    "        add_qkeras_residual(qkeras_quickNet_small, filter_index=i)\n",
    "\n",
    "    # TRANSITION MODULE\n",
    "    add_qkeras_transistion(qkeras_quickNet_small, strides=2, filter_index=i+1)\n",
    "    \n",
    "for i in range(0,4):\n",
    "        add_qkeras_residual(qkeras_quickNet_small, filter_index=3)\n",
    "        \n",
    "# FINAL\n",
    "qkeras_quickNet_small.add(tf.keras.layers.Activation(\"relu\"))\n",
    "qkeras_quickNet_small.add(tf.keras.layers.AveragePooling2D(pool_size=(7,7)))\n",
    "qkeras_quickNet_small.add(tf.keras.layers.Flatten())\n",
    "qkeras_quickNet_small.add(q.QDense(1000, kernel_initializer=\"glorot_normal\"))\n",
    "qkeras_quickNet_small.add(tf.keras.layers.Activation(\"softmax\", dtype=\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = ((32, 64, 256, 512))\n",
    "larq_quickNet_small = tf.keras.models.Sequential() \n",
    "\n",
    "# INPUT\n",
    "larq_quickNet_small.add(tf.keras.layers.InputLayer(input_shape=(224, 224, 3)))\n",
    "\n",
    "# STEM MODULE\n",
    "larq_quickNet_small.add(lq.layers.QuantConv2D(filters[0] // 4, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\", strides=2, use_bias=False))\n",
    "larq_quickNet_small.add(tf.keras.layers.BatchNormalization())\n",
    "larq_quickNet_small.add(tf.keras.layers.Activation(\"relu\"))\n",
    "larq_quickNet_small.add(lq.layers.QuantDepthwiseConv2D((3, 3),padding=\"same\",strides=2, use_bias=False))\n",
    "larq_quickNet_small.add(tf.keras.layers.BatchNormalization(scale=False, center=False))\n",
    "larq_quickNet_small.add(lq.layers.QuantConv2D(filters[0], 1, kernel_initializer=\"he_normal\", use_bias=False))\n",
    "larq_quickNet_small.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "for i in range(0,3):\n",
    "    # RESIDUAL MODULE\n",
    "    for j in range(0,4):\n",
    "        larq_add_qkeras_residual(larq_quickNet_small, filter_index=i)\n",
    "\n",
    "    # TRANSITION MODULE\n",
    "    larq_add_qkeras_transistion(larq_quickNet_small, strides=2, filter_index=i+1)\n",
    "    \n",
    "for i in range(0,4):\n",
    "        larq_add_qkeras_residual(larq_quickNet_small, filter_index=3)\n",
    "        \n",
    "# FINAL\n",
    "larq_quickNet_small.add(tf.keras.layers.Activation(\"relu\"))\n",
    "larq_quickNet_small.add(tf.keras.layers.AveragePooling2D(pool_size=(7,7)))\n",
    "larq_quickNet_small.add(tf.keras.layers.Flatten())\n",
    "larq_quickNet_small.add(lq.layers.QuantDense(1000, kernel_initializer=\"glorot_normal\"))\n",
    "larq_quickNet_small.add(tf.keras.layers.Activation(\"softmax\", dtype=\"float32\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filters = ((64, 128, 256, 512))\n",
    "qkeras_quickNet_large = tf.keras.models.Sequential() \n",
    "\n",
    "# INPUT\n",
    "qkeras_quickNet_large.add(tf.keras.layers.InputLayer(input_shape=(224, 224, 3)))\n",
    "\n",
    "# STEM MODULE\n",
    "qkeras_quickNet_large.add(q.QConv2D(filters[0] // 4, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\", strides=2, use_bias=False))\n",
    "qkeras_quickNet_large.add(tf.keras.layers.BatchNormalization())\n",
    "qkeras_quickNet_large.add(tf.keras.layers.Activation(\"relu\"))\n",
    "qkeras_quickNet_large.add(q.QDepthwiseConv2D((3, 3),padding=\"same\",strides=2, use_bias=False))\n",
    "qkeras_quickNet_large.add(tf.keras.layers.BatchNormalization(scale=False, center=False))\n",
    "qkeras_quickNet_large.add(q.QConv2D(filters[0], 1, kernel_initializer=\"he_normal\", use_bias=False))\n",
    "qkeras_quickNet_large.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "for i in range(0,6):\n",
    "    add_qkeras_residual(qkeras_quickNet_large, filter_index=0)\n",
    "add_qkeras_transistion(qkeras_quickNet_large, strides=2, filter_index=1)\n",
    "\n",
    "for i in range(0,8):\n",
    "    add_qkeras_residual(qkeras_quickNet_large, filter_index=1)\n",
    "add_qkeras_transistion(qkeras_quickNet_large, strides=2, filter_index=2)\n",
    "\n",
    "for i in range(0,12):\n",
    "    add_qkeras_residual(qkeras_quickNet_large, filter_index=2)\n",
    "add_qkeras_transistion(qkeras_quickNet_large, strides=2, filter_index=3)\n",
    "\n",
    "for i in range(0,6):\n",
    "    add_qkeras_residual(qkeras_quickNet_large, filter_index=3)\n",
    "        \n",
    "# FINAL\n",
    "qkeras_quickNet_large.add(tf.keras.layers.Activation(\"relu\"))\n",
    "qkeras_quickNet_large.add(tf.keras.layers.AveragePooling2D(pool_size=(7,7)))\n",
    "qkeras_quickNet_large.add(tf.keras.layers.Flatten())\n",
    "qkeras_quickNet_large.add(q.QDense(1000, kernel_initializer=\"glorot_normal\"))\n",
    "qkeras_quickNet_large.add(tf.keras.layers.Activation(\"softmax\", dtype=\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = ((64, 128, 256, 512))\n",
    "larq_quickNet_large = tf.keras.models.Sequential() \n",
    "\n",
    "# INPUT\n",
    "larq_quickNet_large.add(tf.keras.layers.InputLayer(input_shape=(224, 224, 3)))\n",
    "\n",
    "# STEM MODULE\n",
    "larq_quickNet_large.add(lq.layers.QuantConv2D(filters[0] // 4, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\", strides=2, use_bias=False))\n",
    "larq_quickNet_large.add(tf.keras.layers.BatchNormalization())\n",
    "larq_quickNet_large.add(tf.keras.layers.Activation(\"relu\"))\n",
    "larq_quickNet_large.add(lq.layers.QuantDepthwiseConv2D((3, 3),padding=\"same\",strides=2, use_bias=False))\n",
    "larq_quickNet_large.add(tf.keras.layers.BatchNormalization(scale=False, center=False))\n",
    "larq_quickNet_large.add(lq.layers.QuantConv2D(filters[0], 1, kernel_initializer=\"he_normal\", use_bias=False))\n",
    "larq_quickNet_large.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "for i in range(0,6):\n",
    "    larq_add_qkeras_residual(larq_quickNet_large, filter_index=0)\n",
    "larq_add_qkeras_transistion(larq_quickNet_large, strides=2, filter_index=1)\n",
    "\n",
    "for i in range(0,8):\n",
    "    larq_add_qkeras_residual(larq_quickNet_large, filter_index=1)\n",
    "larq_add_qkeras_transistion(larq_quickNet_large, strides=2, filter_index=2)\n",
    "\n",
    "for i in range(0,12):\n",
    "    larq_add_qkeras_residual(larq_quickNet_large, filter_index=2)\n",
    "larq_add_qkeras_transistion(larq_quickNet_large, strides=2, filter_index=3)\n",
    "\n",
    "for i in range(0,6):\n",
    "    larq_add_qkeras_residual(larq_quickNet_large, filter_index=3)\n",
    "        \n",
    "# FINAL\n",
    "larq_quickNet_large.add(tf.keras.layers.Activation(\"relu\"))\n",
    "larq_quickNet_large.add(tf.keras.layers.AveragePooling2D(pool_size=(7,7)))\n",
    "larq_quickNet_large.add(tf.keras.layers.Flatten())\n",
    "larq_quickNet_large.add(lq.layers.QuantDense(1000, kernel_initializer=\"glorot_normal\"))\n",
    "larq_quickNet_large.add(tf.keras.layers.Activation(\"softmax\", dtype=\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights for qkeras networks\n",
    "qkeras_quickNet.load_weights(path_quicknet)\n",
    "qkeras_quickNet_small.load_weights(path_quicknet_small)\n",
    "qkeras_quickNet_large.load_weights(path_quicknet_large)\n",
    "\n",
    "#load weights for larq networks\n",
    "larq_quickNet.load_weights(path_quicknet)\n",
    "larq_quickNet_small.load_weights(path_quicknet_small)\n",
    "larq_quickNet_large.load_weights(path_quicknet_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MSE(res_qkeras, res_larq):\n",
    "    qres = np.asarray(res_qkeras)\n",
    "    lres = np.asarray(res_larq)    \n",
    "    qres = np.squeeze(qres)\n",
    "    lres = np.squeeze(lres)\n",
    "    mse = []\n",
    "    for i in range(0,qres.shape[0]):\n",
    "        real = lres[i,:]\n",
    "        pred = qres[i,:]\n",
    "        mse.append(((real - pred)**2).mean())\n",
    "    return mse\n",
    "\n",
    "def calculateAbsoluteError(res_qkeras, res_larq):\n",
    "    error_counter = 0\n",
    "    for i, j in zip(res_qkeras, res_larq):\n",
    "        pred = np.argmax(np.asarray(i))\n",
    "        real = np.argmax(np.asarray(j))\n",
    "    if pred != real:\n",
    "        error_counter += 1\n",
    "    return error_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment following line to test with imageNET data\n",
    "#test_data = loadImageNetData(\"../../ILSVRC2012_img_val\", 48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.49s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MSE for quicknet ->  7.661192e-05\n",
      "Absolute errors for quicknet ->  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.30s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MSE for quicknet_small ->  4.548372e-05\n",
      "Absolute errors for quicknet_small ->  0\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4233fe4a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4233fe4a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4251c78440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4251c78440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MSE for quicknet_large ->  0.00035409242\n",
      "Absolute errors for quicknet_large ->  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res_qkeras = []\n",
    "res_larq = []\n",
    "\n",
    "sample_num = 2\n",
    "\n",
    "for i in tqdm(range(0,sample_num)):\n",
    "    res_qkeras.append(qkeras_quickNet.predict(test_data[i,:,:,:]))\n",
    "    res_larq.append(larq_quickNet.predict(test_data[i,:,:,:]))   \n",
    "mse = calculate_MSE(res_qkeras, res_larq)\n",
    "print(\"\\n\\nMSE for quicknet -> \", np.asarray(mse).mean())\n",
    "print(\"Absolute errors for quicknet -> \", calculateAbsoluteError(res_qkeras, res_larq))\n",
    "\n",
    "res_qkeras = []\n",
    "res_larq = []\n",
    "for i in tqdm(range(0,sample_num)):\n",
    "    res_qkeras.append(qkeras_quickNet_small.predict(test_data[i,:,:,:]))\n",
    "    res_larq.append(larq_quickNet_small.predict(test_data[i,:,:,:]))    \n",
    "mse = calculate_MSE(res_qkeras, res_larq)\n",
    "print(\"\\n\\nMSE for quicknet_small -> \", np.asarray(mse).mean())\n",
    "print(\"Absolute errors for quicknet_small -> \", calculateAbsoluteError(res_qkeras, res_larq))\n",
    "\n",
    "res_qkeras = []\n",
    "res_larq = []\n",
    "for i in tqdm(range(0,sample_num)):\n",
    "    res_qkeras.append(qkeras_quickNet_large.predict(test_data[i,:,:,:]))\n",
    "    res_larq.append(larq_quickNet_large.predict(test_data[i,:,:,:]))\n",
    "mse = calculate_MSE(res_qkeras, res_larq)\n",
    "print(\"\\n\\nMSE for quicknet_large -> \", np.asarray(mse).mean())\n",
    "print(\"Absolute errors for quicknet_large -> \", calculateAbsoluteError(res_qkeras, res_larq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... quantizing model\n",
      "  batch_normalization has not been quantized\n",
      "  batch_normalization_1 has not been quantized\n",
      "  batch_normalization_2 has not been quantized\n",
      "  batch_normalization_3 has not been quantized\n",
      "  batch_normalization_4 has not been quantized\n",
      "  batch_normalization_5 has not been quantized\n",
      "  batch_normalization_6 has not been quantized\n",
      "  depthwise_conv2d has not been quantized\n",
      "  batch_normalization_7 has not been quantized\n",
      "  batch_normalization_8 has not been quantized\n",
      "  batch_normalization_9 has not been quantized\n",
      "  batch_normalization_10 has not been quantized\n",
      "  batch_normalization_11 has not been quantized\n",
      "  depthwise_conv2d_1 has not been quantized\n",
      "  batch_normalization_12 has not been quantized\n",
      "  batch_normalization_13 has not been quantized\n",
      "  batch_normalization_14 has not been quantized\n",
      "  batch_normalization_15 has not been quantized\n",
      "  batch_normalization_16 has not been quantized\n",
      "  depthwise_conv2d_2 has not been quantized\n",
      "  batch_normalization_17 has not been quantized\n",
      "  batch_normalization_18 has not been quantized\n",
      "  batch_normalization_19 has not been quantized\n",
      "  batch_normalization_20 has not been quantized\n",
      "  batch_normalization_21 has not been quantized\n",
      "... quantizing model\n",
      "  batch_normalization_44 has not been quantized\n",
      "  batch_normalization_45 has not been quantized\n",
      "  batch_normalization_46 has not been quantized\n",
      "  batch_normalization_47 has not been quantized\n",
      "  batch_normalization_48 has not been quantized\n",
      "  batch_normalization_49 has not been quantized\n",
      "  batch_normalization_50 has not been quantized\n",
      "  depthwise_conv2d_6 has not been quantized\n",
      "  batch_normalization_51 has not been quantized\n",
      "  batch_normalization_52 has not been quantized\n",
      "  batch_normalization_53 has not been quantized\n",
      "  batch_normalization_54 has not been quantized\n",
      "  batch_normalization_55 has not been quantized\n",
      "  depthwise_conv2d_7 has not been quantized\n",
      "  batch_normalization_56 has not been quantized\n",
      "  batch_normalization_57 has not been quantized\n",
      "  batch_normalization_58 has not been quantized\n",
      "  batch_normalization_59 has not been quantized\n",
      "  batch_normalization_60 has not been quantized\n",
      "  depthwise_conv2d_8 has not been quantized\n",
      "  batch_normalization_61 has not been quantized\n",
      "  batch_normalization_62 has not been quantized\n",
      "  batch_normalization_63 has not been quantized\n",
      "  batch_normalization_64 has not been quantized\n",
      "  batch_normalization_65 has not been quantized\n",
      "... quantizing model\n",
      "  batch_normalization_88 has not been quantized\n",
      "  batch_normalization_89 has not been quantized\n",
      "  batch_normalization_90 has not been quantized\n",
      "  batch_normalization_91 has not been quantized\n",
      "  batch_normalization_92 has not been quantized\n",
      "  batch_normalization_93 has not been quantized\n",
      "  batch_normalization_94 has not been quantized\n",
      "  batch_normalization_95 has not been quantized\n",
      "  batch_normalization_96 has not been quantized\n",
      "  depthwise_conv2d_12 has not been quantized\n",
      "  batch_normalization_97 has not been quantized\n",
      "  batch_normalization_98 has not been quantized\n",
      "  batch_normalization_99 has not been quantized\n",
      "  batch_normalization_100 has not been quantized\n",
      "  batch_normalization_101 has not been quantized\n",
      "  batch_normalization_102 has not been quantized\n",
      "  batch_normalization_103 has not been quantized\n",
      "  batch_normalization_104 has not been quantized\n",
      "  batch_normalization_105 has not been quantized\n",
      "  depthwise_conv2d_13 has not been quantized\n",
      "  batch_normalization_106 has not been quantized\n",
      "  batch_normalization_107 has not been quantized\n",
      "  batch_normalization_108 has not been quantized\n",
      "  batch_normalization_109 has not been quantized\n",
      "  batch_normalization_110 has not been quantized\n",
      "  batch_normalization_111 has not been quantized\n",
      "  batch_normalization_112 has not been quantized\n",
      "  batch_normalization_113 has not been quantized\n",
      "  batch_normalization_114 has not been quantized\n",
      "  batch_normalization_115 has not been quantized\n",
      "  batch_normalization_116 has not been quantized\n",
      "  batch_normalization_117 has not been quantized\n",
      "  batch_normalization_118 has not been quantized\n",
      "  depthwise_conv2d_14 has not been quantized\n",
      "  batch_normalization_119 has not been quantized\n",
      "  batch_normalization_120 has not been quantized\n",
      "  batch_normalization_121 has not been quantized\n",
      "  batch_normalization_122 has not been quantized\n",
      "  batch_normalization_123 has not been quantized\n",
      "  batch_normalization_124 has not been quantized\n",
      "  batch_normalization_125 has not been quantized\n"
     ]
    }
   ],
   "source": [
    "# save all models in topology (.txt), weights (.h5) format\n",
    "with lq.context.quantized_scope(True):\n",
    "    larq_quickNet.save(\"./larq_models/larq_quickNet_weights.h5\")  \n",
    "    larq_quickNet_small.save(\"./larq_models/larq_quickNet_small_weights.h5\")\n",
    "    larq_quickNet_large.save(\"./larq_models/larq_quickNet_large_weights.h5\")\n",
    "    \n",
    "larq_quickNet = larq_quickNet.to_json()\n",
    "larq_quickNet_small = larq_quickNet_small.to_json()\n",
    "larq_quickNet_large = larq_quickNet_large.to_json()\n",
    "\n",
    "with open('./larq_models/larq_quickNet.txt', 'w') as outfile:\n",
    "    json.dump(larq_quickNet, outfile)\n",
    "with open('./larq_models/larq_quickNet_small.txt', 'w') as outfile:\n",
    "    json.dump(larq_quickNet_small, outfile)\n",
    "with open('./larq_models/larq_quickNet_large.txt', 'w') as outfile:\n",
    "    json.dump(larq_quickNet_large, outfile)\n",
    "\n",
    "\n",
    "u.model_save_quantized_weights(qkeras_quickNet, filename=\"./qkeras_models/qkeras_quickNet_weights.h5\")\n",
    "u.model_save_quantized_weights(qkeras_quickNet_small, filename=\"./qkeras_models/qkeras_quickNet_small_weights.h5\")\n",
    "u.model_save_quantized_weights(qkeras_quickNet_large, filename=\"./qkeras_models/qkeras_quickNet_large_weights.h5\")\n",
    "\n",
    "qkeras_quickNet = qkeras_quickNet.to_json()\n",
    "qkeras_quickNet_small = qkeras_quickNet_small.to_json()\n",
    "qkeras_quickNet_large = qkeras_quickNet_large.to_json()\n",
    "\n",
    "with open('./qkeras_models/qkeras_quickNet.txt', 'w') as outfile:\n",
    "    json.dump(qkeras_quickNet, outfile)\n",
    "with open('./qkeras_models/qkeras_quickNet_small.txt', 'w') as outfile:\n",
    "    json.dump(qkeras_quickNet_small, outfile)\n",
    "with open('./qkeras_models/qkeras_quickNet_large.txt', 'w') as outfile:\n",
    "    json.dump(qkeras_quickNet_large, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
